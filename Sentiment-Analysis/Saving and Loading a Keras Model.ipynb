{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading a Keras Model\n",
    "\n",
    "As your models get more and more complex, you will find the time it takes to train them quickly starts to increase. They're [significantly faster using a GPU](https://www.analyticsvidhya.com/blog/2017/05/gpus-necessary-for-deep-learning/), so a brilliant option is to train a model once on the fancy OP-VR machine, save it, and then be able to load it from any machine.\n",
    "\n",
    "## Dependencies:\n",
    "* Tensorflow and keras\n",
    "* [HDF5 for Python](http://docs.h5py.org/en/latest/build.html)\n",
    "* numPy\n",
    "\n",
    "## Saving:\n",
    "First we'll make a model to save. This is an example model and could be swapped out. The important bits to note are that the model can be trained and compiled before saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 6s 230us/step - loss: 0.4626 - acc: 0.8092\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 5s 189us/step - loss: 0.2777 - acc: 0.8973\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 5s 203us/step - loss: 0.2315 - acc: 0.9130\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 5s 218us/step - loss: 0.2088 - acc: 0.9220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c8ac7bde10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load dataset from imdb. \n",
    "#This dataset is a collection of movie reviews that are marked as being either a positive or negative review.\n",
    "from keras.datasets import imdb\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "\n",
    "#TOP_WORDS is the top n most frequently used words in the dataset. More uncommon words will be ignored in computations.\n",
    "TOP_WORDS = 5000\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=TOP_WORDS)\n",
    "\n",
    "#The reviews naturally have differing word-counts. \n",
    "#In order to be able to .\n",
    "def vectorize_sequences(sequences, dimension=TOP_WORDS):\n",
    "    #Create an all zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences),dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1. #set specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "#Our vectorised/padded training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test =np.asarray(test_labels).astype('float32')\n",
    "\n",
    "#Build a basic sequential model:\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(TOP_WORDS,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model architecture is saved in a .json file, and the weights are saved separately in a .h5 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "model.save_weights('our_model_weights.h5')\n",
    "\n",
    "# Save the model architecture\n",
    "with open('our_model_architecture.json', 'w') as f:\n",
    "    f.write(model.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to see that these files have been created in your working directory.\n",
    "\n",
    "## Loading:\n",
    "We first load the architecture from the .json file, and then fit the weights from the .h5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                80016     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 80,305\n",
      "Trainable params: 80,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# Model reconstruction from JSON file\n",
    "with open('our_model_architecture.json', 'r') as f:\n",
    "    loaded_model = model_from_json(f.read())\n",
    "\n",
    "# Load weights into the new model\n",
    "loaded_model.load_weights('our_model_weights.h5')\n",
    "\n",
    "loaded_model.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once compiled, we can call methods without re-training, including evaluating test data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 6s 228us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2896669180583954, 0.88288]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
